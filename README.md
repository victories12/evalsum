# evalsum
Re-implement rouge_l, rouge_n, bleu using python. Test them on datasets wikihowAll and DUC2002. Fork me if you like my method.


## Project File Structure

ðŸ“¦project<br>
 â”£ ðŸ“‚data<br>
 â”ƒ â”£ ðŸ“‚DUC2002 --> Store candidate summaries and reference summaries on DUC 2002 <br>
 â”ƒ â”ƒ   â”£ ðŸ“‚candidates --> Store candidates summaries<br>
 â”ƒ â”ƒ   â”£ ðŸ“‚referenceA --> Store No.1 group of reference summaries<br>
 â”ƒ â”ƒ   â”— ðŸ“‚referenceB --> Store No.2 group of reference summaries<br>
 â”ƒ â”— ðŸ“‚wikihowAll --> Store wikihow dataset <br>
 â”ƒ â”ƒ    â”£ extract_wikihowAll.py --> If you want to store the articles and reference summaries here, run this file. Or you could skip this.<br>
 â”ƒ â”ƒ    â”— wikihowAll.csv --> Download from the link and store it here.<br> 
 â”ƒ <br>
 â”£ ðŸ“‚metrics <br>
 â”ƒ â”£ bleu.py --> BLEU metric <br>
 â”ƒ â”£ rouge_l.py --> ROUGE-L metric <br>
 â”ƒ â”— rouge_n.py --> ROUGE-N metric<br>
 â”ƒ <br>
 â”£ ðŸ“‚results --> external tools<br>
 â”ƒ â”£ ðŸ“‚DUC2002 --> models for object detection<br>
 â”ƒ â”ƒ â”ƒ<br>
 â”ƒ â”ƒ â”£ ðŸ“‚A --> Store scores for N0.1 group reference summaries and candidate summaries on DUC 2002<br>
 â”ƒ â”ƒ â”ƒ â”£ BLEU.csv --> Record BLEU scores<br>
 â”ƒ â”ƒ â”ƒ â”£ ROUGE_1.csv --> Record ROUGE_1 scores<br>
 â”ƒ â”ƒ â”ƒ â”£ ROUGE_2.csv --> Record ROUGE_2 scores<br>
 â”ƒ â”ƒ â”ƒ â”— ROUGE_L.csv --> Record ROUGE_L scores<br>
 â”ƒ â”ƒ â”ƒ<br>
 â”ƒ â”ƒ â”— ðŸ“‚B --> Store scores for N0.2 group reference summaries and candidate summaries on DUC 2002<br>
 â”ƒ â”ƒ â”ƒ â”£ BLEU.csv<br>
 â”ƒ â”ƒ â”ƒ â”£ ROUGE_1.csv <br>
 â”ƒ â”ƒ â”ƒ â”£ ROUGE_2.csv<br>
 â”ƒ â”ƒ â”ƒ â”— ROUGE_L.csv <br>
 â”ƒ â”ƒ â”ƒ<br>
 â”ƒ â”ƒ â”£ ðŸ“‚wikihowAll --> Store scores for wikihow dataset <br>
 â”ƒ â”ƒ â”ƒ â”£ BLEU.csv<br>
 â”ƒ â”ƒ â”ƒ â”£ ROUGE_1.csv<br>
 â”ƒ â”ƒ â”ƒ â”£ ROUGE_2.csv <br>
 â”ƒ â”ƒ â”ƒ â”— ROUGE_L.csv<br>
 â”ƒ<br>
 â”£ compare_metrics.py --> Compare between the results of my metrics with the metrics from other package.<br>
 â”£ demo.py --> Test metrics using very simple example to help you understand.<br>
 â”£ run_DUC.py --> Use metrics to evaluate summaries on DUC 2002. The outputs will be stored in results directory. <br>
 â”— run_wikihowAll.py --> Use metrics to evaluate summaries on wikihow.The outputs will be stored in results directory. <br>
 
 ## How to use it
 -check example in the demo.py
```python
from metrics.rouge_l import rouge_l
from metrics.rouge_n import rouge_n
from metrics.bleu import bleu

#rouge
#precision, recall, f_score
hypothesis = "to make people trustworthy you need to trust them"
reference = "the way to make people trustworthy is to trust them"
print(f"(Precison score, Recall score, F_score)")
print(rouge_l([hypothesis],[reference]))
print(rouge_n([hypothesis],[reference],1))
print(rouge_n([hypothesis],[reference],2))


#bleu
hypothesis = "to make people trustworthy you need to trust them"
reference = "the way to make people trustworthy is to trust them"
bleu, precisions, bp, ratio, candidate_length, reference_length = bleu(hypothesis, reference)
print ("BLEU = ",bleu)
print ("BLEU1 = ",precisions[0])
print ("BLEU2 = ",precisions[1])
print ("BLEU3 = ",precisions[2])
print ("BLEU4 = ",precisions[3])
print ("ratio = ",ratio)
``` 

## Test on dataset
-Run run_DUC.py to test metrics on DUC 2002 dataset

-Run run_wikihowAll.py to test metrics on wikihow dataset. Before running, please download wikihowAll.csv from https://ucsb.box.com/s/ap23l8gafpezf4tq3wapr6u8241zz358 and put it to path /data/wikihowAll.
